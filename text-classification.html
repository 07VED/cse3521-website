<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Text classification</title>
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<link rel="stylesheet" href="css/worg.css" type="text/css" media="screen" />
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012  Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="preamble" class="status">
<a href="index.html">Home</a> &nbsp; &nbsp;
          <!-- Plupper Button -->
          <div id="plupperButton" style="display: inline;"></div>
          <!-- End of Plupper Button Code -->
</div>
<div id="content">
<h1 class="title">Text classification</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">Word stemming</a></li>
<li><a href="#sec-2">Feature vectors</a>
<ul>
<li><a href="#sec-2-1">Binary features</a></li>
<li><a href="#sec-2-2">Frequency count features</a></li>
<li><a href="#sec-2-3">Normalized frequency count features</a></li>
</ul>
</li>
<li><a href="#sec-3">tf-idf</a>
<ul>
<li><a href="#sec-3-1">Motivation</a></li>
<li><a href="#sec-3-2">The math</a></li>
</ul>
</li>
<li><a href="#sec-4">k-nearest neighbor</a>
<ul>
<li><a href="#sec-4-1">With binary features</a></li>
<li><a href="#sec-4-2">With frequency count features</a></li>
<li><a href="#sec-4-3">With tf-idf</a></li>
<li><a href="#sec-4-4">Comparing k-nearest neighbor solutions</a></li>
<li><a href="#sec-4-5">Take-away lessons</a></li>
</ul>
</li>
<li><a href="#sec-5">Category centroids</a>
<ul>
<li><a href="#sec-5-1">Benefits of category centroids</a></li>
<li><a href="#sec-5-2">Drawbacks of category centroids</a></li>
</ul>
</li>
<li><a href="#sec-6">Support vector machines</a></li>
</ul>
</div>
</div>
<p>
The goal is to take a new text document (an article from a website,
perhaps) and choose 1, 2, maybe more categories that describe the
document. These documents and categories relate to artificial
intelligence; the documents are obtained from searching the web with
terms like "artificial intelligence," "robot," and "Turing test."
</p>

<p>
The 19 categories are: <a href="http://aaai.org/AITopics/AIOverview">AIOverview</a>, <a href="http://aaai.org/AITopics/Agents">Agents</a>, <a href="http://aaai.org/AITopics/Applications">Applications</a>,
<a href="http://aaai.org/AITopics/CognitiveScience">CognitiveScience</a>, <a href="http://aaai.org/AITopics/Education">Education</a>, <a href="http://aaai.org/AITopics/Ethics">Ethics</a>, <a href="http://aaai.org/AITopics/Games">Games</a>, <a href="http://aaai.org/AITopics/History">History</a>, <a href="http://aaai.org/AITopics/Interfaces">Interfaces</a>,
<a href="http://aaai.org/AITopics/MachineLearning">MachineLearning</a>, <a href="http://aaai.org/AITopics/NaturalLanguage">NaturalLanguage</a>, <a href="http://aaai.org/AITopics/Philosophy">Philosophy</a>, <a href="http://aaai.org/AITopics/Reasoning">Reasoning</a>,
<a href="http://aaai.org/AITopics/Representation">Representation</a>, <a href="http://aaai.org/AITopics/Robots">Robots</a>, <a href="http://aaai.org/AITopics/ScienceFiction">ScienceFiction</a>, <a href="http://aaai.org/AITopics/Speech">Speech</a>, <a href="http://aaai.org/AITopics/Systems">Systems</a>, <a href="http://aaai.org/AITopics/Vision">Vision</a>. These
links go to the AITopics website, which provides a news service called
<i>AI in the News</i> that must solve the problem of categorizing news
articles into one or more of these 19 categories.
</p>

<p>
These notes describe how to do the categorization; obtaining documents
and their content is beyond our scope. Thus, we will utilize a
prepared collection of documents. The zip files linked here provide
about 3000 documents with human-chosen categories. We'll take a
portion of these, say 90%, and train our categorization tools. The
remaining 10% will be used for testing purposes. Which 90% and 10% are
training and testing sets should be set randomly (and differently in
perhaps 5 or 10 different experiments to ensure a categorizer's
performance is robust).
</p>

<p>
<a href="./downloads/cse630-ai-articles.zip">Zip file of all AI articles</a> &#x2014; these documents have the raw original text.
</p>

<p>
<a href="./downloads/cse630-ai-articles-stemmed.zip">Zip file of all AI articles (stemmed documents)</a> &#x2014; these documents
are the same as above, but every word in the text has been "stemmed"
(see below). This is the document set we will be working with.
</p>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">Word stemming</h2>
<div class="outline-text-2" id="text-1">
<blockquote>
<p>
In linguistic morphology and information retrieval, stemming is the
process for reducing inflected (or sometimes derived) words to their
stem, base or root form&#x2014;generally a written word form. The stem need
not be identical to the morphological root of the word; it is usually
sufficient that related words map to the same stem, even if this stem
is not in itself a valid root.
</p>

<p>
[&#x2026;]
</p>

<p>
A stemmer for English, for example, should identify the string "cats"
(and possibly "catlike", "catty" etc.) as based on the root "cat", and
"stemmer", "stemming", "stemmed" as based on "stem". A stemming
algorithm reduces the words "fishing", "fished", "fish", and "fisher"
to the root word, "fish". (<a href="http://en.wikipedia.org/wiki/Stemming">Wikipedia</a>)
</p>
</blockquote>
</div>
</div>
<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2">Feature vectors</h2>
<div class="outline-text-2" id="text-2">
<p>
A document \(j\) is represented as:
</p>

<p>
$$X_j = (x_{j1}, x_{j2}, \dots, x_{jc}),$$
</p>

<p>
where \(c\) is the count of unique words across all documents in the
database. Thus, all documents have vectors of the same number of
dimensions (\(c\) dimensions).
</p>
</div>

<div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1">Binary features</h3>
<div class="outline-text-3" id="text-2-1">
<p>
$$x_{ji} = \cases{1 & \text{if \(j\) is in $i$} \cr 0 & \text{otherwise}}$$
</p>
</div>
</div>
<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2">Frequency count features</h3>
<div class="outline-text-3" id="text-2-2">
<p>
$$x_{ji} = tf_{ji},$$
</p>

<p>
where \(tf_{ji}\) is the frequency (count) of the term in the document.
</p>
</div>
</div>
<div id="outline-container-sec-2-3" class="outline-3">
<h3 id="sec-2-3">Normalized frequency count features</h3>
<div class="outline-text-3" id="text-2-3">
<p>
$$x_{ji} = \frac{tf_{ji}}{\sqrt{\sum_{k} tf_{ki}^2}}$$
</p>
</div>
</div>
</div>
<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3">tf-idf</h2>
<div class="outline-text-2" id="text-3">
<p>
The document vectors described above all suffer from over-scoring
common words. A common word like "the" does not give any evidence that
one document is closely related to another one just because they both
contain many instances of that word. So in addition to recording how
often a term appears in a document, we need to divide out some measure
of how common is the term across different documents. A term appearing
in nearly every document is not as relevant as a term appearing in
few. Additionally, if a rare term appears more often in a document, it
is even more indicative of the content of that document.
</p>
</div>

<div id="outline-container-sec-3-1" class="outline-3">
<h3 id="sec-3-1">Motivation</h3>
<div class="outline-text-3" id="text-3-1">
<blockquote>
<p>
Suppose we have a set of English text documents and wish to determine
which document is most relevant to the query "the brown cow." A simple
way to start out is by eliminating documents that do not contain all
three words "the", "brown", and "cow", but this still leaves many
documents. To further distinguish them, we might count the number of
times each term occurs in each document and sum them all together; the
number of times a term occurs in a document is called its term
frequency.
</p>

<p>
However, because the term "the" is so common, this will tend to
incorrectly emphasize documents which happen to use the word "the"
more frequently, without giving enough weight to the more meaningful
terms "brown" and "cow". The term "the" is not a good keyword to
distinguish relevant and non-relevant documents and terms, unlike the
less common words "brown" and "cow". Hence an inverse document
frequency factor is incorporated which diminishes the weight of terms
that occur very frequently in the collection and increases the weight
of terms that occur rarely. (<a href="http://en.wikipedia.org/wiki/Tf*idf">Wikipedia</a>)
</p>
</blockquote>
</div>
</div>
<div id="outline-container-sec-3-2" class="outline-3">
<h3 id="sec-3-2">The math</h3>
<div class="outline-text-3" id="text-3-2">
<p>
Every dimension (word) in the document vector has the following value:
</p>

<p>
$$x_{ji} = \log(tf_{ji} + 1) \log(n/df_j + 1),$$
</p>

<p>
where \(j\) is the index of a word, \(i\) is the index of a document,
\(x_{ji}\) is the value in position \(j\) (for word \(j\)) in the vector
representing document \(i\), \(tf_{ji}\) is the frequency (count) of the
term \(j\) in the document \(i\), \(n\) is the number of documents in the
database, and \(df_j\) is the number of documents in the database that
contain at least one occurrence of the term \(j\).
</p>

<div class="center">

<div class="figure">
<p><img src="./images/tfidf-graph.png" alt="tfidf-graph.png" />
</p>
</div>
</div>

<p>
This graph shows the tf-idf value for a particular term in a document
that comes from a database of 1,010 documents. The bottom axis is \(df\)
(document frequency for that term); the top numbered axis (the "depth"
axis) is \(tf\) or (term frequency; the number of times that term
appears in the document under consideration); the right axis is the
tf-idf value.
</p>

<p>
We see in the graph that an uncommon term (\(df\) is low) produces a
higher tf-idf value. However, due to the use of logarithms, the tf-idf
value grows little as the term appears more often in the document. In
all, a term that's appears several times in the document and is
somewhat unique to the document is a strong indicator (of category
membership, of relevance to a query containing that term, etc.).
</p>

<p>
An example of binary, frequency, and tfidf document vectors can be
found in the <a href="./doc-vectors.xls">doc-vectors.xls</a> spreadsheet.
</p>
</div>
</div>
</div>
<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4">k-nearest neighbor</h2>
<div class="outline-text-2" id="text-4">
<p>
Since we have different ways of representing the document vectors, we
have different "distance" calculations. Below, take \(\hat{X} =
(\hat{x}_0, \dots, \hat{x}_m)\) to be the document we want to classify,
and \(X_j = (x_{j0}, \dots, x_{jn})\) to be a document from the database
(\(j\) will range so we check the distance to every document in the
database).
</p>
</div>

<div id="outline-container-sec-4-1" class="outline-3">
<h3 id="sec-4-1">With binary features</h3>
<div class="outline-text-3" id="text-4-1">
<p>
$$d = \frac{\sum_{i} \hat{x}_{i} x_{ji}}{\sum_{i} \hat{x}_{i}}$$
</p>

<p>
The numerator of the fraction gives the number of shared words; the
denominator gives the number of words in the document to be
classified. Thus, \(0 \leq d \leq 1\).
</p>
</div>
</div>
<div id="outline-container-sec-4-2" class="outline-3">
<h3 id="sec-4-2">With frequency count features</h3>
<div class="outline-text-3" id="text-4-2">
<p>
$$d = \sqrt{\sum_{i} (\hat{x}_{i} - x_{ji})^2}$$
</p>

<p>
This is just Euclidean distance between the two document
vectors. Thus, \(0 \leq d < \infty\).
</p>
</div>
</div>
<div id="outline-container-sec-4-3" class="outline-3">
<h3 id="sec-4-3">With tf-idf</h3>
<div class="outline-text-3" id="text-4-3">
<p>
$$d = \sum_{i} \hat{x}_{i} x_{ji},$$
</p>

<p>
assuming the vectors \(\hat{X}\) (the document we are trying to
classify) and \(X_j\) (each document in the database) have been
normalized so that each of their Euclidean distances to the origin
vector (all zeros) is equal to 1.0.
</p>

<p>
The above is a dot-product of two vectors. Because these vectors have
been normalized, the dot-product can also be called the cosine
similarity:
</p>

<p>
$$\cos \theta = \hat{X} \cdot X_j,$$
</p>

<p>
where \(0 \leq \theta \leq \pi/2\) and \(\theta\) is the angle (from the
origin) between the two document vectors in the higher-dimensional
space. More similar documents have a smaller angle between them,
meaning their cosine similarity is closer to 1.0. Documents with
absolutely no terms in common have a dot-product equal to 0.0, or
equivalently, a 90-degree angle between them.
</p>
</div>
</div>
<div id="outline-container-sec-4-4" class="outline-3">
<h3 id="sec-4-4">Comparing k-nearest neighbor solutions</h3>
<div class="outline-text-3" id="text-4-4">
<div class="center">

<div class="figure">
<p><img src="./images/classification-alg-comparison.png" alt="classification-alg-comparison.png" />
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-sec-4-5" class="outline-3">
<h3 id="sec-4-5">Take-away lessons</h3>
<div class="outline-text-3" id="text-4-5">
<ul class="org-ul">
<li>Good choices for what I am calling the "algorithm" (tf-idf, count
with normalization, etc.) depends on the dataset. However, <b>tf-idf
is robust</b> and works best on all datasets represented here. This
is a significant result.
</li>

<li>Using a weighted voting technique is generally a good idea.
</li>

<li>Increasing \(k\) does not always increase performance.
</li>

<li>The best value for \(k\) depends on the the dataset and the
algorithm.
</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-sec-5" class="outline-2">
<h2 id="sec-5">Category centroids</h2>
<div class="outline-text-2" id="text-5">
<p>
To construct the "centroid" (average, mean) of a category, we gather
up all the documents that are members of the category, and compute the
mean of their document vectors:
</p>

<p>
$$C_c = \frac{\sum_j X_j^c}{n_c},$$
</p>

<p>
where \(C_c\) is the centroid for category \(c\), \(X_j^c\) is a document
vector from the category \(c\), and \(n_c\) is the number of documents in
the category.
</p>

<p>
When we have a new test document that we want to classify, we compute
its distance from each category centroid. Then, we can either choose
to take the "closest" or "most similar" category, or the best \(n\)
categories, or all categories that have a distance less than some
threshold.
</p>

<div class="center">

<div class="figure">
<p><img src="./images/corpus-mds-centroids.png" alt="corpus-mds-centroids.png" />
</p>
</div>
</div>

<p>
This is a plot (after applying multidimensional scaling) of how
"different" each category centroid is from each other. It seems that
Ethics, AIOverview, and History are quite similar, for example.
</p>

<div class="center">

<div class="figure">
<p><img src="./images/centroid-fscore-datasets.png" alt="centroid-fscore-datasets.png" />
</p>
</div>
</div>

<p>
Data from 10 experiments are shown in these <a href="http://en.wikipedia.org/wiki/Box_plot">box plots</a>. Performance is
not great on any dataset.
</p>

<div class="center">

<div class="figure">
<p><img src="./images/centroid-fscore-ai.png" alt="centroid-fscore-ai.png" />
</p>
</div>
</div>

<p>
The AI dataset often has multiple category membership (the other
datasets always have one category per document), so we have to choose
how many categories to select. We can use a threshold (choose all
categories where the similarity of the document with the category
centroid is greater than a threshold), or we can choose the best \(n\)
categories. The plot above shows performance for this second approach,
using various values for \(n\). For all such values, performance is
quite poor.
</p>
</div>

<div id="outline-container-sec-5-1" class="outline-3">
<h3 id="sec-5-1">Benefits of category centroids</h3>
<div class="outline-text-3" id="text-5-1">
<ul class="org-ul">
<li>The centroids are computed ahead of time, and when we get a new
document, we only need to compute its distance to a handful of
category centroids (rather than compute its distance to every
document in the database).
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-5-2" class="outline-3">
<h3 id="sec-5-2">Drawbacks of category centroids</h3>
<div class="outline-text-3" id="text-5-2">
<ul class="org-ul">
<li>An averaged document vector may be distorted due to the presence
of outliers (very bizarre documents that are members of the
category). Perhaps some category outlier detector can remove these
documents from the calculation of the mean.
</li>

<li>When using a dot-product technique, say \(A \cdot B\), if a vector
\(B\) has more non-zero terms than another vector \(B'\) then the
dot-product \(A \cdot B\) has a good chance of being greater than \(A
    \cdot B'\). This means that categories with more documents will
produce centroids with more non-zero terms (because the documents
in the category span more words than a category with fewer
documents), and therefore, everything else being equal, larger
dot-products. So the category with the most members will be chosen
most often. (N.B., this is all conjecture; I have not verified
this suspicion.)
</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-sec-6" class="outline-2">
<h2 id="sec-6">Support vector machines</h2>
<div class="outline-text-2" id="text-6">
<p>
We will not be learning about support vector machines in this class;
the topic is too advanced. However, it should be noted that they
perform <i>very</i> well:
</p>

<div class="center">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="left" />

<col  class="right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="left">Dataset (with tfidf)</th>
<th scope="col" class="right">Average F-score</th>
</tr>
</thead>
<tbody>
<tr>
<td class="left">ai</td>
<td class="right">0.925</td>
</tr>

<tr>
<td class="left">la1</td>
<td class="right">0.950</td>
</tr>

<tr>
<td class="left">oh10</td>
<td class="right">0.957</td>
</tr>

<tr>
<td class="left">re0</td>
<td class="right">0.974</td>
</tr>

<tr>
<td class="left">tr21</td>
<td class="right">0.953</td>
</tr>

<tr>
<td class="left">wap</td>
<td class="right">0.981</td>
</tr>
</tbody>
</table>
</div>

<p>
However, support vector machine training is very slow: training the
"la1" dataset (only 2883 documents and 6 categories) took 2.5 hours on
a quad-core machine (using all cores). Training with the naïve Bayes
method (described in <a href="./naive-bayesian.html">Naïve Bayesian classification</a> notes) takes only a
few seconds.
</p>

<div style="font-size: 80%; clear: both;"> <span
xmlns:dct="http://purl.org/dc/terms/"
href="http://purl.org/dc/dcmitype/Text" property="dct:title"
rel="dct:type">Intro to AI material</span> by <a
xmlns:cc="http://creativecommons.org/ns#"
href="http://cse3521.artifice.cc" property="cc:attributionName"
rel="cc:attributionURL">Joshua Eckroth</a> is licensed under a <a
rel="license"
href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons
Attribution-ShareAlike 3.0 Unported License</a>. Source code for this
website available at <a
href="https://github.com/joshuaeckroth/cse3521-website">GitHub</a>.
</div>

<!-- Plupper Tracking Code -->
<script src="https://www.google.com/jsapi"></script>
<script type="text/javascript"
    src="https://static.plupper.com/js/plupper.js"></script>
<script type="text/javascript">
    plupper.init("joshuaeckroth@plupper.com");
    plupper.enableCobrowsing();
</script>
<!-- End of Plupper Tracking Code -->

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-23971505-3', 'artifice.cc');
  ga('send', 'pageview');

</script>

<script src="http://code.jquery.com/jquery-1.9.1.js"></script>
<script src="http://code.jquery.com/ui/1.10.3/jquery-ui.js"></script>

<script type="text/javascript">
$('.hidden .hidden-content').hide();
$('.hidden > strong').click(function() {
  $(this).parent().find('.hidden-content').toggle();
});
</script>
</div>
</div>
</div>
</body>
</html>
