<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Naïve Bayesian classification</title>
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<link rel="stylesheet" href="css/worg.css" type="text/css" media="screen" />
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012  Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="preamble" class="status">
<a href="index.html">Home</a> &nbsp; &nbsp;
          <!-- Plupper Button -->
          <div id="plupperButton" style="display: inline;"></div>
          <!-- End of Plupper Button Code -->
</div>
<div id="content">
<h1 class="title">Naïve Bayesian classification</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">Feature vectors</a></li>
<li><a href="#sec-2">Category vectors</a></li>
<li><a href="#sec-3">Algorithm</a></li>
<li><a href="#sec-4">A problem with tiny values</a></li>
<li><a href="#sec-5">Evaluation</a></li>
<li><a href="#sec-6">Benefits of naïve Bayes</a></li>
<li><a href="#sec-7">Drawbacks of naïve Bayes</a></li>
</ul>
</div>
</div>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">Feature vectors</h2>
<div class="outline-text-2" id="text-1">
<p>
Documents are simply binary word vectors. No tf-idf transformation is
done.
</p>
</div>
</div>
<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2">Category vectors</h2>
<div class="outline-text-2" id="text-2">
<p>
Each category vector is represented as a series of probabilities, one
probability per word (each vector dimension represents a word, just
like a document feature vector). Each probability means, "the
probability of this word being present in a document that is a member
of this category." Thus, the category vector has terms \(C_c = (p_{c1},
p_{c2}, \dots, p_{ck})\), and
</p>

<p>
$$p_{ci} = P(w_i|C_c) = \frac{d_{ci}+1}{d_{i}+|C|},$$
</p>

<p>
where \(d_{ci}\) is the number of documents in \(C_c\) that have word \(i\)
(anywhere in the document, any number of occurrences), \(d_i\) is the
number of documents in <i>any</i> category that have word \(i\), and \(|C|\) is
the number of categories. We add 1 and add \(|C|\) so that \(P(w_i|C_c)\)
is never equal to 0. (This is called <a href="http://en.wikipedia.org/wiki/Additive_smoothing">Laplace smoothing</a>.)
</p>
</div>
</div>
<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3">Algorithm</h2>
<div class="outline-text-2" id="text-3">
<p>
We assume, for simplicity, that the occurrences of words in documents
are completely independent (this is what makes the method
"naïve"). This is patently false since, for instance, the words
"vision" and "image" often both appear in documents about computer
vision; so seeing the word "vision" suggests that "image" will also
appear in the document.
</p>

<p>
We further assume that the order the words appear in the document does
not matter.
</p>

<p>
Because we make this independence assumption, we can calculate the
probability of a document being a member of some category quite
easily:
</p>

<p>
$$P(\hat{X}|C_c) = \prod_i P(w_i|C_c),$$
</p>

<p>
where \(P(w_i|C_c) = p_{ci}\) (from the definition above).
</p>

<p>
Now, Bayes' theorem gives us:
</p>

<p>
$$P(C_c|\hat{X}) = P(\hat{X}|C_c)P(C_c) / P(\hat{X}),$$
</p>

<p>
with,
</p>

<p>
$$P(C_c) = \frac{n_c + 1}{n + |C|},$$
</p>

<p>
where \(n_c\) is the number of documents in category \(C_c\) and \(n\) is
the number of documents overall. Again, we use Laplace smoothing; this
allows us to avoid probabilities equal to 0.0.
</p>

<p>
Since we want to find the category \(C_c\) that makes the quantity
maximal, we can ignore \(P(\hat{X})\) because it does not change
depending on which category we are considering.
</p>

<p>
Thus, we are actually looking for:
</p>

<p>
$$\arg\max_{C_c} P(C_c|\hat{X}) = \arg\max_{C_c} P(\hat{X}|C_c)P(C_c)$$
</p>

<p>
We just check all the categories, and choose the single best or top \(N\).
</p>
</div>
</div>
<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4">A problem with tiny values</h2>
<div class="outline-text-2" id="text-4">
<p>
With a lot of unique words, we create very small values by multiplying
many \(p_{ci}\) terms. On a computer, the values may become so small
that they may "underflow" (run out of bits required to represent the
value). To prevent this, we just throw a logarithm around everything:
</p>

<p>
$$\log P(\hat{X}|C_c)P(C_c) = \log P(\hat{X}|C_c) + \log P(C_c),$$
</p>

<p>
and furthermore,
</p>

<p>
$$\log P(\hat{X}|C_c) = \log \prod_i P(w_i|C_c) = \sum_i \log P(w_i|C_c)$$
</p>

<p>
So our multiplications turn to sums, and we avoid the underflow
problem. Rewriting again, we ultimately have this problem:
</p>

<p>
$$\arg\max_{C_c} (\sum_i \log P(w_i|C_c)) + \log P(C_c)$$
</p>
</div>
</div>
<div id="outline-container-sec-5" class="outline-2">
<h2 id="sec-5">Evaluation</h2>
<div class="outline-text-2" id="text-5">
<p>
These graphs show the performance of the naïve Bayes approach on
various datasets (compare with results from the <a href="./document-classification.html">document
classification</a> notes). The calculations described above are
represented as the "binary" algorithm in the graphs. The "tfidf"
algorithm, as applied in a naïve Bayes context, uses slightly
different calculations that have not been described in these notes.
</p>

<div class="center">

<div class="figure">
<p><img src="./images/naivebayes-fscore-datasets.png" alt="naivebayes-fscore-datasets.png" />
</p>
</div>
</div>

<div class="center">

<div class="figure">
<p><img src="./images/naivebayes-fscore-ai.png" alt="naivebayes-fscore-ai.png" />
</p>
</div>
</div>

<p>
The book <a href="http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html">Introduction to Information Retrieval</a> gathered some published
results for classification tasks. We can see that naïve Bayes is
usually not as good as k-nearest neighbor (which we did learn about)
nor support vector machines (which we didn't learn about).
</p>

<div class="center">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="left" />

<col  class="right" />

<col  class="right" />

<col  class="right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="left">Dataset</th>
<th scope="col" class="right">Naïve Bayes</th>
<th scope="col" class="right">k-nearest neighbor</th>
<th scope="col" class="right">Support vector machines</th>
</tr>
</thead>
<tbody>
<tr>
<td class="left">earn</td>
<td class="right">0.96</td>
<td class="right">0.97</td>
<td class="right">0.98</td>
</tr>

<tr>
<td class="left">acq</td>
<td class="right">0.88</td>
<td class="right">0.92</td>
<td class="right">0.94</td>
</tr>

<tr>
<td class="left">money-fx</td>
<td class="right">0.57</td>
<td class="right">0.78</td>
<td class="right">0.75</td>
</tr>

<tr>
<td class="left">grain</td>
<td class="right">0.79</td>
<td class="right">0.82</td>
<td class="right">0.95</td>
</tr>

<tr>
<td class="left">crude</td>
<td class="right">0.80</td>
<td class="right">0.86</td>
<td class="right">0.89</td>
</tr>

<tr>
<td class="left">trade</td>
<td class="right">0.64</td>
<td class="right">0.77</td>
<td class="right">0.76</td>
</tr>

<tr>
<td class="left">interest</td>
<td class="right">0.65</td>
<td class="right">0.74</td>
<td class="right">0.78</td>
</tr>

<tr>
<td class="left">ship</td>
<td class="right">0.85</td>
<td class="right">0.79</td>
<td class="right">0.86</td>
</tr>

<tr>
<td class="left">wheat</td>
<td class="right">0.70</td>
<td class="right">0.77</td>
<td class="right">0.92</td>
</tr>

<tr>
<td class="left">corn</td>
<td class="right">0.65</td>
<td class="right">0.78</td>
<td class="right">0.90</td>
</tr>
</tbody>
</table>
</div>

<p>
I also performed a Spam detection experiment, using the <a href="http://archive.ics.uci.edu/ml/datasets/Spambase">Spambase</a>
dataset. With naïve Bayes I was able to achieve ~80% accuracy.
</p>
</div>
</div>
<div id="outline-container-sec-6" class="outline-2">
<h2 id="sec-6">Benefits of naïve Bayes</h2>
<div class="outline-text-2" id="text-6">
<ul class="org-ul">
<li>It is very fast. In the table above, while naïve Bayes does not
perform as well, it is significantly more efficient than either
k-nearest neighbor or support vector machines. The latter, support
vector machines, are <i>painfully</i> slow (at least in the training
phase).
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-7" class="outline-2">
<h2 id="sec-7">Drawbacks of naïve Bayes</h2>
<div class="outline-text-2" id="text-7">
<ul class="org-ul">
<li>Accuracy is low, as seen in the table above.
</li>
</ul>


<div style="font-size: 80%; clear: both;">
<span xmlns:dct="http://purl.org/dc/terms/" href="http://purl.org/dc/dcmitype/Text" property="dct:title" rel="dct:type">CSE 3521 material</span> by <a xmlns:cc="http://creativecommons.org/ns#" href="http://cse3521.artifice.cc" property="cc:attributionName" rel="cc:attributionURL">Joshua Eckroth</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>. Source code for this website available at <a href="https://github.com/joshuaeckroth/cse3521-website">GitHub</a>.
</div>

<!-- Plupper Tracking Code -->
<script src="https://www.google.com/jsapi"></script>
<script type="text/javascript"
    src="https://static.plupper.com/js/plupper.js"></script>
<script type="text/javascript">
    plupper.init("joshuaeckroth@plupper.com");
    plupper.enableCobrowsing();
</script>
<!-- End of Plupper Tracking Code -->
</div>
</div>
</div>
</body>
</html>
