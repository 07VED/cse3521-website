<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Feature detection</title>
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<link rel="stylesheet" href="css/worg.css" type="text/css" media="screen" />
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012  Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="preamble" class="status">
<a href="index.html">Home</a> &nbsp; &nbsp;
          <!-- Plupper Button -->
          <div id="plupperButton" style="display: inline;"></div>
          <!-- End of Plupper Button Code -->
</div>
<div id="content">
<h1 class="title">Feature detection</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">Step 1: Detect features</a></li>
<li><a href="#sec-2">Step 2: Create feature vectors ("descriptors")</a></li>
<li><a href="#sec-3">Step 3: Match features across images</a></li>
<li><a href="#sec-4">Step 4: Keep only "good" matches</a></li>
<li><a href="#sec-5">Step 5: Get the keypoints of the good matches</a></li>
<li><a href="#sec-6">Step 5: Find the homography across the keypoints</a></li>
</ul>
</div>
</div>
<div class="center">

<div class="figure">
<p><img src="./images/SURF_Homography.jpg" alt="SURF_Homography.jpg" />
</p>
</div>
</div>

<blockquote>
<p>
The task of finding point correspondences between two images of the
same scene or object is part of many computer vision
applications. Image registration, camera calibration, object
recognition, and image retrieval are just a few.
</p>

<p>
The search for discrete image point correspondences can be divided
into three main steps. First, 'interest points' are selected at
distinctive locations in the image, such as corners, blobs, and
T-junctions. The most valuable property of an interest point
<i>detector</i> is its repeatability. The repeatability expresses the
reliability of a detector for finding the same physical interest
points under different viewing conditions. Next, the neighbourhood of
every interest point is represented by a feature vector. This
<i>descriptor</i> has to be distinctive and at the same time robust to
noise, detection displacements and geometric and photometric
deformations. Finally, the descriptor vectors are <i>matched</i> between
different images. The matching is based on a distance between the
vectors, e.g. the Mahalanobis or Euclidean distance. The dimension of
the descriptor has a direct impact on the time this takes, and less
dimensions are desirable for fast interest point matching. However,
lower dimensional feature vectors are in general less distinctive than
their high-dimensional counterparts.
</p>

<p>
It has been our goal to develop both a detector and descriptor that,
in comparison to the state-of-the-art, are fast to compute while not
sacrificing performance. In order to succeed, one has to strike a
balance between the above requirements like simplifying the detection
scheme while keeping it accurate, and reducing the descriptor's size
while keeping it sufficiently distinctive.
</p>

<p>
Introductory text of Herbert Bay, Andreas Ess, Tinne Tuytelaars, Luc
Van Gool, <a href="ftp://ftp.vision.ee.ethz.ch/publications/articles/eth_biwi_00517.pdf">"SURF: Speeded Up Robust Features"</a>, Computer Vision and
Image Understanding (CVIU), Vol. 110, No. 3, pp. 346&#x2013;359, 2008
</p>
</blockquote>

<p>
A good overview: <a href="http://mesh.brown.edu/engn1610/szeliski/04-featuredetectionandmatching.pdf">Computer Vision: Algorithms and Applications, Ch 4</a> by
Richard Szeliski.
</p>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">Step 1: Detect features</h2>
<div class="outline-text-2" id="text-1">
<p>
(what is a feature)
</p>

<p>
SURF: "Speeded-Up Robust Features"
</p>

<div class="org-src-container">

<pre class="src src-c++"><span style="color: #999999; background-color: #000000;">SurfFeatureDetector</span> <span style="color: #999999; background-color: #000000;">detector</span>;

<span style="color: #999999; background-color: #000000;">std</span>::<span style="color: #999999; background-color: #000000;">vector</span>&lt;KeyPoint&gt; <span style="color: #999999; background-color: #000000;">keypoints_object</span>, <span style="color: #999999; background-color: #000000;">keypoints_scene</span>;

detector.detect(img_object, keypoints_object);
detector.detect(img_scene, keypoints_scene);
</pre>
</div>
</div>
</div>
<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2">Step 2: Create feature vectors ("descriptors")</h2>
<div class="outline-text-2" id="text-2">
<p>
(what is a descriptor i.e. keypoint)
</p>

<div class="org-src-container">

<pre class="src src-c++"><span style="color: #999999; background-color: #000000;">SurfDescriptorExtractor</span> <span style="color: #999999; background-color: #000000;">extractor</span>;

<span style="color: #999999; background-color: #000000;">Mat</span> <span style="color: #999999; background-color: #000000;">descriptors_object</span>, <span style="color: #999999; background-color: #000000;">descriptors_scene</span>;

extractor.compute(img_object, keypoints_object, descriptors_object);
extractor.compute(img_scene, keypoints_scene, descriptors_scene);
</pre>
</div>
</div>
</div>
<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3">Step 3: Match features across images</h2>
<div class="outline-text-2" id="text-3">
<p>
(nearest-neighbor search: each scene keypoint is matched with an
object image keypoint based on the keypoints' descriptor vectors)
</p>

<div class="org-src-container">

<pre class="src src-c++"><span style="color: #999999; background-color: #000000;">FlannBasedMatcher</span> <span style="color: #999999; background-color: #000000;">matcher</span>;
<span style="color: #999999; background-color: #000000;">std</span>::<span style="color: #999999; background-color: #000000;">vector</span>&lt;DMatch&gt; <span style="color: #999999; background-color: #000000;">matches</span>;
matcher.match(descriptors_object, descriptors_scene, matches);
</pre>
</div>
</div>
</div>
<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4">Step 4: Keep only "good" matches</h2>
<div class="outline-text-2" id="text-4">
<p>
First, find the minimum and maximum distances between keypoints.
</p>

<div class="org-src-container">

<pre class="src src-c++"><span style="color: #999999; background-color: #000000;">double</span> <span style="color: #999999; background-color: #000000;">max_dist</span> = 0;
<span style="color: #999999; background-color: #000000;">double</span> <span style="color: #999999; background-color: #000000;">min_dist</span> = 100;

<span style="color: #999999; background-color: #000000; font-weight: bold;">for</span>( <span style="color: #999999; background-color: #000000;">int</span> <span style="color: #999999; background-color: #000000;">i</span> = 0; i &lt; descriptors_object.rows; i++ )
{
    <span style="color: #999999; background-color: #000000;">double</span> <span style="color: #999999; background-color: #000000;">dist</span> = matches[i].distance;
    <span style="color: #999999; background-color: #000000; font-weight: bold;">if</span>(dist &lt; min_dist) min_dist = dist;
    <span style="color: #999999; background-color: #000000; font-weight: bold;">if</span>(dist &gt; max_dist) max_dist = dist;
}
</pre>
</div>

<p>
Get the "good" matches.
</p>

<div class="org-src-container">

<pre class="src src-c++"><span style="color: #999999; background-color: #000000;">std</span>::<span style="color: #999999; background-color: #000000;">vector</span>&lt;DMatch&gt; <span style="color: #999999; background-color: #000000;">good_matches</span>;

<span style="color: #999999; background-color: #000000; font-weight: bold;">for</span>(<span style="color: #999999; background-color: #000000;">int</span> <span style="color: #999999; background-color: #000000;">i</span> = 0; i &lt; descriptors_object.rows; i++)
{
    <span style="color: #999999; background-color: #000000; font-weight: bold;">if</span>(matches[i].distance &lt; 3*min_dist)
    {
        good_matches.push_back(matches[i]);
    }
}
</pre>
</div>

<p>
Draw the matches on top of the images.
</p>

<div class="org-src-container">

<pre class="src src-c++"><span style="color: #999999; background-color: #000000;">Mat</span> <span style="color: #999999; background-color: #000000;">img_matches</span>; <span style="color: #444444; background-color: #000000;">// </span><span style="color: #444444; background-color: #000000;">resulting image</span>
drawMatches(img_object, keypoints_object, img_scene, keypoints_scene,
            good_matches, img_matches, Scalar(0, 0, 255));
</pre>
</div>
</div>
</div>
<div id="outline-container-sec-5" class="outline-2">
<h2 id="sec-5">Step 5: Get the keypoints of the good matches</h2>
<div class="outline-text-2" id="text-5">
<div class="org-src-container">

<pre class="src src-c++"><span style="color: #999999; background-color: #000000;">std</span>::<span style="color: #999999; background-color: #000000;">vector</span>&lt;Point2f&gt; <span style="color: #999999; background-color: #000000;">obj</span>;
<span style="color: #999999; background-color: #000000;">std</span>::<span style="color: #999999; background-color: #000000;">vector</span>&lt;Point2f&gt; <span style="color: #999999; background-color: #000000;">scene</span>;

<span style="color: #999999; background-color: #000000; font-weight: bold;">for</span>(<span style="color: #999999; background-color: #000000;">int</span> <span style="color: #999999; background-color: #000000;">i</span> = 0; i &lt; good_matches.size(); i++)
{
    <span style="color: #444444; background-color: #000000;">// </span><span style="color: #444444; background-color: #000000;">queryIdx and trainIdx allow us to get the original points</span>
    <span style="color: #444444; background-color: #000000;">// </span><span style="color: #444444; background-color: #000000;">of our good matches by referring back to the keypoints arrays</span>
    obj.push_back(keypoints_object[good_matches[i].queryIdx].pt);
    scene.push_back(keypoints_scene[good_matches[i].trainIdx].pt);
}
</pre>
</div>
</div>
</div>
<div id="outline-container-sec-6" class="outline-2">
<h2 id="sec-6">Step 5: Find the homography across the keypoints</h2>
<div class="outline-text-2" id="text-6">
<p>
RANSAC method, from <a href="http://en.wikipedia.org/wiki/RANSAC">Wikipedia</a>:
</p>

<blockquote>
<p>
RANSAC is an abbreviation for "RANdom SAmple Consensus". It is an
iterative method to estimate parameters of a mathematical model from a
set of observed data which contains outliers. It is a
non-deterministic algorithm in the sense that it produces a reasonable
result only with a certain probability, with this probability
increasing as more iterations are allowed.
</p>

<p>
The input to the RANSAC algorithm is a set of observed data values, a
parameterized model which can explain or be fitted to the
observations, and some confidence parameters.
</p>

<p>
RANSAC achieves its goal by iteratively selecting a random subset of
the original data. These data are hypothetical inliers and this
hypothesis is then tested as follows:
</p>

<ol class="org-ol">
<li>A model is fitted to the hypothetical inliers, i.e. all free
parameters of the model are reconstructed from the inliers.
</li>

<li>All other data are then tested against the fitted model and, if a
point fits well to the estimated model, also considered as a
hypothetical inlier.
</li>

<li>The estimated model is reasonably good if sufficiently many points
have been classified as hypothetical inliers.
</li>

<li>The model is reestimated from all hypothetical inliers, because it
has only been estimated from the initial set of hypothetical
inliers.
</li>

<li>Finally, the model is evaluated by estimating the error of the
inliers relative to the model. However, this might lead the
algorithm picking a trivial solution, with a minimal set of inliers
(as the model will result in zero error estimation). Instead, the
"best model" is typically chosen as the one with the largest
consensus set
</li>
</ol>

<p>
This procedure is repeated a fixed number of times, each time
producing either a model which is rejected because too few points are
classified as inliers or a refined model together with a corresponding
error measure. In the latter case, we keep the refined model if its
error is lower than the last saved model.
</p>
</blockquote>

<div class="org-src-container">

<pre class="src src-c++"><span style="color: #999999; background-color: #000000;">Mat</span> <span style="color: #999999; background-color: #000000;">H</span> = findHomography(obj, scene, CV_RANSAC);
</pre>
</div>

<p>
Transform the boundaries in the object image to the scene image, using
the homography.
</p>

<div class="org-src-container">

<pre class="src src-c++"><span style="color: #999999; background-color: #000000;">std</span>::<span style="color: #999999; background-color: #000000;">vector</span>&lt;Point2f&gt; <span style="color: #999999; background-color: #000000; font-weight: bold; text-decoration: underline;">obj_corners</span>(4);
obj_corners[0] = cvPoint(0,0);
obj_corners[1] = cvPoint(img_object.cols, 0);
obj_corners[2] = cvPoint(img_object.cols, img_object.rows);
obj_corners[3] = cvPoint(0, img_object.rows);
<span style="color: #999999; background-color: #000000;">std</span>::<span style="color: #999999; background-color: #000000;">vector</span>&lt;Point2f&gt; <span style="color: #999999; background-color: #000000; font-weight: bold; text-decoration: underline;">scene_corners</span>(4);

perspectiveTransform(obj_corners, scene_corners, H);
</pre>
</div>

<p>
Draw the transformed boundaries in the scene.
</p>

<div class="org-src-container">

<pre class="src src-c++">line(img_matches,
     scene_corners[0] + Point2f(img_object.cols, 0),
     scene_corners[1] + Point2f(img_object.cols, 0),
     Scalar(0, 255, 0), 4);
line(img_matches,
     scene_corners[1] + Point2f(img_object.cols, 0),
     scene_corners[2] + Point2f(img_object.cols, 0),
     Scalar(0, 255, 0), 4);
line(img_matches,
     scene_corners[2] + Point2f(img_object.cols, 0),
     scene_corners[3] + Point2f(img_object.cols, 0),
     Scalar(0, 255, 0), 4);
line(img_matches,
     scene_corners[3] + Point2f(img_object.cols, 0),
     scene_corners[0] + Point2f(img_object.cols, 0),
     Scalar(0, 255, 0), 4);
</pre>
</div>

<div style="font-size: 80%; clear: both;"> <span
xmlns:dct="http://purl.org/dc/terms/"
href="http://purl.org/dc/dcmitype/Text" property="dct:title"
rel="dct:type">Intro to AI material</span> by <a
xmlns:cc="http://creativecommons.org/ns#"
href="http://cse3521.artifice.cc" property="cc:attributionName"
rel="cc:attributionURL">Joshua Eckroth</a> is licensed under a <a
rel="license"
href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons
Attribution-ShareAlike 3.0 Unported License</a>. Source code for this
website available at <a
href="https://github.com/joshuaeckroth/cse3521-website">GitHub</a>.
</div>

<!-- Plupper Tracking Code -->
<script src="https://www.google.com/jsapi"></script>
<script type="text/javascript"
    src="https://static.plupper.com/js/plupper.js"></script>
<script type="text/javascript">
    plupper.init("joshuaeckroth@plupper.com");
    plupper.enableCobrowsing();
</script>
<!-- End of Plupper Tracking Code -->

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-23971505-3', 'artifice.cc');
  ga('send', 'pageview');

</script>

<script src="http://code.jquery.com/jquery-1.9.1.js"></script>
<script src="http://code.jquery.com/ui/1.10.3/jquery-ui.js"></script>

<script type="text/javascript">
$('.hidden .hidden-content').hide();
$('.hidden > strong').click(function() {
  $(this).parent().find('.hidden-content').toggle();
});
</script>
</div>
</div>
</div>
</body>
</html>
